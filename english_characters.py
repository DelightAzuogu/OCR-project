# -*- coding: utf-8 -*-
"""English_characters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pf4Jh60LdIdiOIYPn46MdwIHcxh32q1t
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
dhruvildave_english_handwritten_characters_dataset_path = kagglehub.dataset_download('dhruvildave/english-handwritten-characters-dataset')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input , Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
import cv2 as cv
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler
from tensorflow.keras.optimizers import Adam
from sklearn.utils import shuffle

import matplotlib.pyplot as plt

test_data = '/content/english (1).csv'
df = pd.read_csv(test_data)

df["label"].value_counts()

# labels = df_test_data['label'].tolist()
# labels

images_path = '/kaggle/input/english-handwritten-characters-dataset/Img'

# List to hold the dimensions of all images
image_sizes = []

for image_file in os.listdir(images_path):
    image_path = os.path.join(images_path, image_file)
    image = cv.imread(image_path)
    if image is not None:
        height, width = image.shape[:2]
        image_sizes.append((width, height))

# Print the size of the first image
if image_sizes:
    print("Size of the first image:", image_sizes[1])
    print("Unique image sizes:", set(image_sizes))
else:
    print("No images found in the directory.")

images_path = '/kaggle/input/english-handwritten-characters-dataset/Img'
images = []
labels = []

for image_file in os.listdir(images_path):
    image_path = os.path.join(images_path, image_file)
    image = cv.imread(image_path).astype('float32')
    image = cv.resize(image, (64, 64))
    image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)
    #image = cv.GaussianBlur(image, (1, 1), 0)
    image /= 255.0
    label_index = os.path.join("Img", image_file)
    label = df[df['image'] == label_index].label.values[0]
    images.append(image)
    labels.append(label)

images = np.array(images)
labels = np.array(labels)

plt.gray()
plt.imshow(images[100])

print("Number of images:", len(images))
print("Shape of each image:", images.shape[1:])
print("Unique labels:", np.unique(labels))

# One hot encode the labels

encoder = OneHotEncoder(sparse_output=False)
labels = encoder.fit_transform(labels.reshape(-1, 1))

decoder = [str(i) for i in range(10)] + [chr(i) for i in range(65, 91)] + [chr(i) for i in range(97, 123)]

images, labels = shuffle(images, labels, random_state=0)

from tensorflow.keras.callbacks import LearningRateScheduler
import numpy as np

# Define an exponential decay learning rate schedule function
def exponential_decay(epoch, lr):
    initial_lr = 0.001
    k = 0.1
    decay_rate = k ** (epoch / 10)
    return initial_lr * decay_rate

X_train, X_temp, Y_train, Y_temp = train_test_split(images, labels, test_size=0.3, random_state=0)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=0)

datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.2,
    zoom_range=0.2,
    shear_range=0.2,
    fill_mode='nearest'
)

# Initialize lists to hold augmented images and labels
augmented_arr_X = []
augmented_arr_Y = []
number_of_augmentation = 5

# Iterate over each image in X_train
for i in range(len(X_train)):
    # Ensure the image has the correct shape (height, width, channels)
    image = X_train[i]
    if image.ndim == 2:
        image = np.expand_dims(image, axis=-1)  # Convert (height, width) to (height, width, channels)

    # Expand dimensions to include batch size
    image_expanded = np.expand_dims(image, axis=0)  # Shape: (1, height, width, channels)

    # Create iterator for augmented images
    aug_iter = datagen.flow(image_expanded, batch_size=1)

    # Generate augmented images
    augmented_images = [next(aug_iter)[0] for _ in range(number_of_augmentation)]

    # Extend the list with the new augmented images and corresponding labels
    augmented_arr_X.extend(augmented_images)
    augmented_arr_Y.extend([Y_train[i]] * number_of_augmentation)

# Convert lists to numpy arrays
augmented_arr_X = np.array(augmented_arr_X)
augmented_arr_Y = np.array(augmented_arr_Y)

print("Augmented images shape:", augmented_arr_X.shape)
print("Augmented labels shape:", augmented_arr_Y.shape)

X_train = np.array(augmented_arr_X)
Y_train = np.array(augmented_arr_Y)

X_train, Y_train = shuffle(X_train, Y_train, random_state=0)

# Plot 9 random images for visualisation

def display_images(rows, cols, images, labels, decoder):
    _, axes = plt.subplots(nrows = 3, ncols = 3, figsize=(12,6))

    for i, ax in enumerate(axes.flatten()):
        ax.imshow(images[i], cmap='gray')
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_title(f"Label: {decoder[np.where(labels[i]==1)[0][0]]}")

display_images(3, 3, images, labels, decoder)

num_classes = len(np.unique(labels))  # Check how many unique labels you have
print("Number of classes:", num_classes)

# Define model

model = Sequential([
    Input(shape=(64, 64, 1)),

    Conv2D(512, (5, 5), activation='relu'),

    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(256, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(256, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),

    Dense(512, activation='relu'),
    Dropout(0.1),

    Dense(1024, activation='relu'),

    Dense(512, activation='relu'),
    Dropout(0.1),

    Dense(256, activation='sigmoid'),

    Dense(62, activation='softmax')
])

model.summary()

print("Shape of X_train:", X_train.shape)
print("Shape of X_val:", X_val.shape)
print("Shape of X_test:", X_test.shape)

import tensorflow as tf

from tensorflow.keras.mixed_precision import global_policy, set_global_policy, Policy

# Create a mixed precision policy
policy = Policy('mixed_float16')
set_global_policy(policy)

def scheduler(epoch, lr):
    if epoch < 3:
        return lr
    else:
        return float(lr * tf.math.exp(-0.1))


lr_scheduler = LearningRateScheduler(scheduler)

# Define early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Compile the model
model.compile(optimizer=Adam(learning_rate=1e-4),  # Use a lower learning rate for stability
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model with the callbacks
history = model.fit(
    X_train, Y_train,
    epochs=5,
    batch_size=32,
    validation_data=(X_val, Y_val),
    callbacks=[lr_scheduler, early_stopping]  # Add both callbacks here
)

model.save('final_model.h5')

import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping
from tensorflow.keras.mixed_precision import Policy, set_global_policy

# === Set Mixed Precision Policy ===
policy = Policy('mixed_float16')
set_global_policy(policy)

# === Load the Model ===
model = load_model('/content/final_model.h5')

# === Learning Rate Scheduler ===
def scheduler(epoch, lr):
    if epoch < 3:
        return lr
    else:
        return float(lr * tf.math.exp(-0.1))

lr_scheduler = LearningRateScheduler(scheduler)

# === Early Stopping Callback ===
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# === Compile the Model ===
# Explicitly re-define the optimizer because the state isn't loaded.
model.compile(
    optimizer=Adam(learning_rate=1e-4),  # Ensure you're using the same optimizer configuration
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# === Continue Training ===
history = model.fit(
    X_train, Y_train,
    epochs=30,  # Updated to 20
    batch_size=32,
    validation_data=(X_val, Y_val),
    callbacks=[lr_scheduler, early_stopping]
)

import tensorflow as tf
import numpy as np
import cv2
import os

# Load the trained model
model = tf.keras.models.load_model('/content/final_model.h5')  # replace with your model's file path

# Define the image path for prediction
image_path = "/content/jay.png"

# Load the image (ensure it's grayscale, as expected by the model)
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# Resize the image to 64x64 (this matches the training preprocessing)
image = cv2.resize(image, (64, 64))

# Normalize the image to the range [0, 1]
image = image.astype("float32") / 255.0

# Add a batch dimension (making it shape (1, 64, 64, 1))
image = np.expand_dims(image, axis=-1)  # shape becomes (64, 64, 1)
image = np.expand_dims(image, axis=0)   # shape becomes (1, 64, 64, 1)

# Make the prediction
predictions = model.predict(image)

# Get the predicted class (for classification models)
predicted_class = np.argmax(predictions, axis=-1)

print(f"Predicted class: {predicted_class}")

import tensorflow as tf
import numpy as np
import cv2
import matplotlib.pyplot as plt

# Define the class labels
class_labels = [
    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H',
    'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',
    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r',
    's', 't', 'u', 'v', 'w', 'x', 'y', 'z'
]

# Load the trained model
model = tf.keras.models.load_model('/content/final_model.h5')  # replace with your model's file path

# Define the image path for prediction
image_path = "/content/six.png"

# Load the image (ensure it's grayscale, as expected by the model)
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# Resize the image to 64x64 (this matches the training preprocessing)
image_resized = cv2.resize(image, (64, 64))

# Normalize the image to the range [0, 1]
image_resized = image_resized.astype("float32") / 255.0

# Add a batch dimension (making it shape (1, 64, 64, 1))
image_resized = np.expand_dims(image_resized, axis=-1)  # shape becomes (64, 64, 1)
image_resized = np.expand_dims(image_resized, axis=0)   # shape becomes (1, 64, 64, 1)

# Make the prediction
predictions = model.predict(image_resized)

# Get the predicted index (for classification models)
predicted_index = np.argmax(predictions, axis=-1)

# Get the predicted class label
predicted_label = class_labels[predicted_index[0]]

# Display the image and predicted label
plt.imshow(image, cmap='gray')  # Show the original image
plt.title(f"Predicted: {predicted_label}")  # Display the predicted label as the title
plt.axis('off')  # Hide axes for a cleaner image display
plt.show()

print(f"Predicted class: {predicted_label}")